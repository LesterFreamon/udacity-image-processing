{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Classification\n",
    "---\n",
    "In this notebook, we define **and train** an CNN to classify images from the [Fashion-MNIST database](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the [data](http://pytorch.org/docs/master/torchvision/datasets.html)\n",
    "\n",
    "In this cell, we load in both **training and test** datasets from the FashionMNIST class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T16:56:05.421588Z",
     "start_time": "2024-05-02T16:55:55.229257Z"
    }
   },
   "source": [
    "# our basic libraries\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# data loading and transforming\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors for input into a CNN\n",
    "\n",
    "## Define a transform to read the data in as a tensor\n",
    "data_transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = FashionMNIST(root='./data', train=True,\n",
    "                                   download=True, transform=data_transform)\n",
    "\n",
    "test_data = FashionMNIST(root='./data', train=False,\n",
    "                                  download=True, transform=data_transform)\n",
    "\n",
    "\n",
    "# Print out some stats about the training and test data\n",
    "print('Train data, number of images: ', len(train_data))\n",
    "print('Test data, number of images: ', len(test_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:04<00:00, 6148740.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 201404.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:01<00:00, 2995833.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 9533014.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Train data, number of images:  60000\n",
      "Test data, number of images:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T17:02:02.445407Z",
     "start_time": "2024-05-02T17:02:02.441302Z"
    }
   },
   "source": [
    "# prepare data loaders, set the batch_size\n",
    "## TODO: you can try changing the batch_size to be larger or smaller\n",
    "## when you get to training your network, see how batch_size affects the loss\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some training data\n",
    "\n",
    "This cell iterates over the training dataset, loading a random batch of image/label data, using `dataiter.next()`. It then plots the batch of images and labels in a `2 x batch_size/2` grid."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:07:00.267242Z",
     "start_time": "2024-05-02T17:06:59.937840Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# obtain one batch of training images\u001B[39;00m\n\u001B[1;32m      7\u001B[0m dataiter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(train_loader)\n\u001B[0;32m----> 8\u001B[0m images, labels \u001B[38;5;241m=\u001B[39m \u001B[43mdataiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext\u001B[49m()\n\u001B[1;32m      9\u001B[0m images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# plot the images in the batch, along with the corresponding labels\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:07:08.173073Z",
     "start_time": "2024-05-02T17:07:08.157725Z"
    }
   },
   "cell_type": "code",
   "source": "type(images)",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mtype\u001B[39m(\u001B[43mimages\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'images' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture\n",
    "\n",
    "The various layers that make up any neural network are documented, [here](http://pytorch.org/docs/master/nn.html). For a convolutional neural network, we'll use a simple series of layers:\n",
    "* Convolutional layers\n",
    "* Maxpooling layers\n",
    "* Fully-connected (linear) layers\n",
    "\n",
    "You are also encouraged to look at adding [dropout layers](http://pytorch.org/docs/stable/nn.html#dropout) to avoid overfitting this data.\n",
    "\n",
    "---\n",
    "\n",
    "To define a neural network in PyTorch, you define the layers of a model in the function `__init__` and define the feedforward behavior of a network that employs those initialized layers in the function `forward`, which takes in an input image tensor, `x`. The structure of this Net class is shown below and left for you to fill in.\n",
    "\n",
    "Note: During training, PyTorch will be able to perform backpropagation by keeping track of the network's feedforward behavior and using autograd to calculate the update to the weights in the network.\n",
    "\n",
    "#### Define the Layers in ` __init__`\n",
    "As a reminder, a conv/pool layer may be defined like this (in `__init__`):\n",
    "```\n",
    "# 1 input image channel (for grayscale images), 32 output channels/feature maps, 3x3 square convolution kernel\n",
    "self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "\n",
    "# maxpool that uses a square window of kernel_size=2, stride=2\n",
    "self.pool = nn.MaxPool2d(2, 2)      \n",
    "```\n",
    "\n",
    "#### Refer to Layers in `forward`\n",
    "Then referred to in the `forward` function like this, in which the conv1 layer has a ReLu activation applied to it before maxpooling is applied:\n",
    "```\n",
    "x = self.pool(F.relu(self.conv1(x)))\n",
    "```\n",
    "\n",
    "You must place any layers with trainable weights, such as convolutional layers, in the `__init__` function and refer to them in the `forward` function; any layers or functions that always behave in the same way, such as a pre-defined activation function, may appear *only* in the `forward` function. In practice, you'll often see conv/pool layers defined in `__init__` and activations defined in `forward`.\n",
    "\n",
    "#### Convolutional layer\n",
    "The first convolution layer has been defined for you, it takes in a 1 channel (grayscale) image and outputs 10 feature maps as output, after convolving the image with 3x3 filters.\n",
    "\n",
    "#### Flattening\n",
    "\n",
    "Recall that to move from the output of a convolutional/pooling layer to a linear layer, you must first flatten your extracted features into a vector. If you've used the deep learning library, Keras, you may have seen this done by `Flatten()`, and in PyTorch you can flatten an input `x` with `x = x.view(x.size(0), -1)`.\n",
    "\n",
    "### TODO: Define the rest of the layers\n",
    "\n",
    "It will be up to you to define the other layers in this network; we have some recommendations, but you may change the architecture and parameters as you see fit.\n",
    "\n",
    "Recommendations/tips:\n",
    "* Use at least two convolutional layers\n",
    "* Your output must be a linear layer with 10 outputs (for the 10 classes of clothing)\n",
    "* Use a dropout layer to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:17:33.580032Z",
     "start_time": "2024-05-02T17:17:33.569480Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input image channel (grayscale), 10 output channels/feature maps\n",
    "        # 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3) \n",
    "        \n",
    "        ## TODO: Define the rest of the layers:\n",
    "        # include another conv layer, maxpooling layers, and linear layers\n",
    "        # also consider adding a dropout layer to avoid overfitting\n",
    "        self.conv2 = nn.Conv2d(10, 32, 3, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dense = nn.Linear(6*6*32, 10)\n",
    "        \n",
    "        \n",
    "\n",
    "    ## TODO: define the feedforward behavior\n",
    "    def forward(self, x):\n",
    "        # one activated conv layer\n",
    "        x = F.relu(self.conv1(x)) # -> 26x26x10\n",
    "        x = self.pool(F.relu(self.conv2(x))) # -> 12x12x32 -> 6x6x32\n",
    "        x = x.view(x.size(0), -1) # -> 1152\n",
    "        x = F.relu(self.dense(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        # final output\n",
    "        return x\n",
    "\n",
    "# instantiate and print your Net\n",
    "net = Net()\n",
    "print(net)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dense): Linear(in_features=1152, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Specify the loss function and optimizer\n",
    "\n",
    "Learn more about [loss functions](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizers](http://pytorch.org/docs/master/optim.html) in the online documentation.\n",
    "\n",
    "Note that for a classification problem like this, one typically uses cross entropy loss, which can be defined in code like: `criterion = nn.CrossEntropyLoss()`. PyTorch also includes some standard stochastic optimizers like stochastic gradient descent and Adam. You're encouraged to try different optimizers and see how your model responds to these choices as it trains.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T17:22:34.718213Z",
     "start_time": "2024-05-02T17:22:34.713594Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "## TODO: specify loss function (try categorical cross-entropy)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "## TODO: specify optimizer \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on accuracy\n",
    "\n",
    "It's interesting to look at the accuracy of your network **before and after** training. This way you can really see that your network has learned something. In the next cell, let's see what the accuracy of an untrained network is (we expect it to be around 10% which is the same accuracy as just guessing for all 10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:22:36.095093Z",
     "start_time": "2024-05-02T17:22:35.124275Z"
    }
   },
   "source": [
    "# Calculate accuracy before training\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through test dataset\n",
    "for images, labels in test_loader:\n",
    "\n",
    "    # forward pass to get outputs\n",
    "    # the outputs are a series of class scores\n",
    "    outputs = net(images)\n",
    "\n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # count up total number of correct labels\n",
    "    # for which the predicted and true labels are equal\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# print it out!\n",
    "print('Accuracy before training: ', accuracy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training:  tensor(10.)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network\n",
    "\n",
    "Below, we've defined a `train` function that takes in a number of epochs to train for. The number of epochs is how many times a network will cycle through the training dataset. \n",
    "\n",
    "Here are the steps that this training function performs as it iterates over the training dataset:\n",
    "\n",
    "1. Zero's the gradients to prepare for a forward pass\n",
    "2. Passes the input through the network (forward pass)\n",
    "3. Computes the loss (how far is the predicted classes are from the correct labels)\n",
    "4. Propagates gradients back into the network’s parameters (backward pass)\n",
    "5. Updates the weights (parameter update)\n",
    "6. Prints out the calculated loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T17:22:36.741915Z",
     "start_time": "2024-05-02T17:22:36.736571Z"
    }
   },
   "source": [
    "def train(n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            inputs, labels = data       \n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "            running_loss += loss.item()\n",
    "            if batch_i % 1000 == 999:    # print every 1000 mini-batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:23:17.202492Z",
     "start_time": "2024-05-02T17:22:37.845915Z"
    }
   },
   "source": [
    "# define the number of epochs to train for\n",
    "n_epochs = 5 # start small to see if your model works, initially\n",
    "\n",
    "# call train\n",
    "train(n_epochs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 1, Batch: 2000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 1, Batch: 3000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 2, Batch: 1000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 2, Batch: 2000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 2, Batch: 3000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 3, Batch: 1000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 3, Batch: 2000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 3, Batch: 3000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 4, Batch: 1000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 4, Batch: 2000, Avg. Loss: 2.3025853633880615\n",
      "Epoch: 4, Batch: 3000, Avg. Loss: 2.3025853633880615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m n_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;66;03m# start small to see if your model works, initially\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# call train\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[23], line 14\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(n_epochs)\u001B[0m\n\u001B[1;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# forward pass to get outputs\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# calculate the loss\u001B[39;00m\n\u001B[1;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[9], line 25\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# one activated conv layer\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;66;03m# -> 26x26x10\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x))) \u001B[38;5;66;03m# -> 12x12x32 -> 6x6x32\u001B[39;00m\n\u001B[1;32m     27\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;66;03m# -> 1152\u001B[39;00m\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Trained Network\n",
    "\n",
    "Once you are satisfied with how the loss of your model has decreased, there is one last step: test!\n",
    "\n",
    "You must test your trained model on a previously unseen dataset to see if it generalizes well and can accurately classify this new dataset. For FashionMNIST, which contains many pre-processed training images, a good model should reach **greater than 85% accuracy** on this test dataset. If you are not reaching this value, try training for a larger number of epochs, tweaking your hyperparameters, or adding/subtracting layers from your CNN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:19:24.859795Z",
     "start_time": "2024-05-02T17:19:23.977685Z"
    }
   },
   "source": [
    "# initialize tensor and lists to monitor test loss and accuracy\n",
    "test_loss = torch.zeros(1)\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# set the module to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for batch_i, data in enumerate(test_loader):\n",
    "    \n",
    "    # get the input images and their corresponding labels\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # forward pass to get outputs\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "            \n",
    "    # update average test loss \n",
    "    test_loss = test_loss + ((torch.ones(1) / (batch_i + 1)) * (loss.data - test_loss))\n",
    "    \n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(predicted.eq(labels.data.view_as(predicted)))\n",
    "    \n",
    "    # calculate test accuracy for *each* object class\n",
    "    # we get the scalar value of correct items for a class, by calling `correct[i].item()`\n",
    "    for i in range(batch_size):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss.numpy()[0]))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "        \n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.806390\n",
      "\n",
      "Test Accuracy of T-shirt/top: 93% (938/1000)\n",
      "Test Accuracy of Trouser:  0% ( 0/1000)\n",
      "Test Accuracy of Pullover:  0% ( 0/1000)\n",
      "Test Accuracy of Dress: 95% (959/1000)\n",
      "Test Accuracy of  Coat: 92% (924/1000)\n",
      "Test Accuracy of Sandal:  0% ( 0/1000)\n",
      "Test Accuracy of Shirt:  0% ( 0/1000)\n",
      "Test Accuracy of Sneaker:  0% ( 0/1000)\n",
      "Test Accuracy of   Bag:  0% ( 0/1000)\n",
      "Test Accuracy of Ankle boot:  0% ( 0/1000)\n",
      "\n",
      "Test Accuracy (Overall): 28% (2821/10000)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample test results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T17:20:34.957695Z",
     "start_time": "2024-05-02T17:20:34.546594Z"
    }
   },
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "# get predictions\n",
    "preds = np.squeeze(net(images).data.max(1, keepdim=True)[1].numpy())\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of columns must be a positive integer, not 10.0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m fig \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m25\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39marange(batch_size):\n\u001B[0;32m---> 11\u001B[0m     ax \u001B[38;5;241m=\u001B[39m \u001B[43mfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_subplot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxticks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myticks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     ax\u001B[38;5;241m.\u001B[39mimshow(np\u001B[38;5;241m.\u001B[39msqueeze(images[idx]), cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgray\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     13\u001B[0m     ax\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(classes[preds[idx]], classes[labels[idx]]),\n\u001B[1;32m     14\u001B[0m                  color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgreen\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m preds[idx]\u001B[38;5;241m==\u001B[39mlabels[idx] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mred\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/matplotlib/figure.py:768\u001B[0m, in \u001B[0;36mFigureBase.add_subplot\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    765\u001B[0m         args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mstr\u001B[39m(args[\u001B[38;5;241m0\u001B[39m])))\n\u001B[1;32m    766\u001B[0m     projection_class, pkw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_projection_requirements(\n\u001B[1;32m    767\u001B[0m         \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 768\u001B[0m     ax \u001B[38;5;241m=\u001B[39m \u001B[43mprojection_class\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    769\u001B[0m     key \u001B[38;5;241m=\u001B[39m (projection_class, pkw)\n\u001B[1;32m    770\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_axes_internal(ax, key)\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/matplotlib/axes/_base.py:644\u001B[0m, in \u001B[0;36m_AxesBase.__init__\u001B[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001B[0m\n\u001B[1;32m    642\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_position \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_originalPosition \u001B[38;5;241m=\u001B[39m mtransforms\u001B[38;5;241m.\u001B[39mBbox\u001B[38;5;241m.\u001B[39munit()\n\u001B[0;32m--> 644\u001B[0m     subplotspec \u001B[38;5;241m=\u001B[39m \u001B[43mSubplotSpec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_subplot_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_position\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_position\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWidth and height specified must be non-negative\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/matplotlib/gridspec.py:589\u001B[0m, in \u001B[0;36mSubplotSpec._from_subplot_args\u001B[0;34m(figure, args)\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    587\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _api\u001B[38;5;241m.\u001B[39mnargs_error(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubplot\u001B[39m\u001B[38;5;124m\"\u001B[39m, takes\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 or 3\u001B[39m\u001B[38;5;124m\"\u001B[39m, given\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(args))\n\u001B[0;32m--> 589\u001B[0m gs \u001B[38;5;241m=\u001B[39m \u001B[43mGridSpec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_gridspec_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfigure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    591\u001B[0m     gs \u001B[38;5;241m=\u001B[39m GridSpec(rows, cols, figure\u001B[38;5;241m=\u001B[39mfigure)\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/matplotlib/gridspec.py:226\u001B[0m, in \u001B[0;36mGridSpecBase._check_gridspec_exists\u001B[0;34m(figure, nrows, ncols)\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m gs\n\u001B[1;32m    225\u001B[0m \u001B[38;5;66;03m# else gridspec not found:\u001B[39;00m\n\u001B[0;32m--> 226\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGridSpec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mncols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfigure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfigure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/matplotlib/gridspec.py:379\u001B[0m, in \u001B[0;36mGridSpec.__init__\u001B[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhspace \u001B[38;5;241m=\u001B[39m hspace\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure \u001B[38;5;241m=\u001B[39m figure\n\u001B[0;32m--> 379\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mncols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mwidth_ratios\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwidth_ratios\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mheight_ratios\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheight_ratios\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/virtual_envs/env_full_stack/lib/python3.10/site-packages/matplotlib/gridspec.py:52\u001B[0m, in \u001B[0;36mGridSpecBase.__init__\u001B[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of rows must be a positive integer, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnrows\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ncols, Integral) \u001B[38;5;129;01mor\u001B[39;00m ncols \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 52\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     53\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of columns must be a positive integer, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mncols\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nrows, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ncols \u001B[38;5;241m=\u001B[39m nrows, ncols\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_height_ratios(height_ratios)\n",
      "\u001B[0;31mValueError\u001B[0m: Number of columns must be a positive integer, not 10.0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2500x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are some weaknesses of your model? (And how might you improve these in future iterations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Double-click and write answer, here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Save Your Best Model\n",
    "\n",
    "Once you've decided on a network architecture and are satisfied with the test accuracy of your model after training, it's time to save this so that you can refer back to this model, and use it at a later data for comparison of for another classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: change the model_name to something uniqe for any new model\n",
    "## you wish to save, this will save it in the saved_models directory\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'model_1.pt'\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "# when you're ready, un-comment the line below\n",
    "# torch.save(net.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Trained, Saved Model\n",
    "\n",
    "To instantiate a trained model, you'll first instantiate a new `Net()` and then initialize it with a saved dictionary of parameters (from the save step above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your Net\n",
    "# this refers to your Net class defined above\n",
    "net = Net()\n",
    "\n",
    "# load the net parameters by name\n",
    "# uncomment and write the name of a saved model\n",
    "#net.load_state_dict(torch.load('saved_models/model_1.pt'))\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Once you've loaded a specific model in, you can then \n",
    "# us it or further analyze it! \n",
    "# This will be especialy useful for feature visualization "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
